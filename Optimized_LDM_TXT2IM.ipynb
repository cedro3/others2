{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimized LDM-TXT2IM",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/others2/blob/main/Optimized_LDM_TXT2IM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Diffusion Models Text2Image\n",
        "\n",
        "### https://arxiv.org/abs/2112.10752\n",
        "\n",
        "### https://github.com/CompVis/latent-diffusion\n",
        "\n",
        "Original Notebook by: [Eyal Gruss](https://eyalgruss.com) \\([@eyaler](https://twitter.com/eyaler)\\)\n",
        "\n",
        "Optimizations by: [Aaron Gokaslan](https://twitter.com/SkyLi0n) and faster sampling by [RiverHasWings](https://twitter.com/rivershavewings)"
      ],
      "metadata": {
        "id": "Bmvx0uTbF6Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note you need a GPU with 16GB of VRAM. If you get a K80, try again."
      ],
      "metadata": {
        "id": "g1QAMXzKCUO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "X8L4kLA9Ad2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "%cd /content\n",
        "!git clone https://github.com/crowsonkb/latent-diffusion --depth 1\n",
        "!git clone https://github.com/CompVis/taming-transformers --depth 1\n",
        "!pip -q install -e ./taming-transformers\n",
        "!pip -q install omegaconf pytorch-lightning torch-fidelity einops transformers\n",
        "%cd latent-diffusion\n",
        "!cp scripts/txt2img.py .\n",
        "!mkdir -p models/ldm/text2img-large/\n",
        "!wget -nc -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n"
      ],
      "metadata": {
        "id": "2iLdwkKD5l8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile txt2img.py\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm, trange\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--prompt\",\n",
        "        type=str,\n",
        "        nargs=\"?\",\n",
        "        default=\"a painting of a virus monster playing guitar\",\n",
        "        help=\"the prompt to render\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--outdir\",\n",
        "        type=str,\n",
        "        nargs=\"?\",\n",
        "        help=\"dir to write results to\",\n",
        "        default=\"outputs/txt2img-samples\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--ddim_steps\",\n",
        "        type=int,\n",
        "        default=200,\n",
        "        help=\"number of ddim sampling steps\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--plms\",\n",
        "        action='store_true',\n",
        "        help=\"use plms sampling\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--ddim_eta\",\n",
        "        type=float,\n",
        "        default=0.0,\n",
        "        help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--n_iter\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"sample this often\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--H\",\n",
        "        type=int,\n",
        "        default=256,\n",
        "        help=\"image height, in pixel space\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--W\",\n",
        "        type=int,\n",
        "        default=256,\n",
        "        help=\"image width, in pixel space\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--n_samples\",\n",
        "        type=int,\n",
        "        default=4,\n",
        "        help=\"how many samples to produce for the given prompt\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--scale\",\n",
        "        type=float,\n",
        "        default=5.0,\n",
        "        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n",
        "    )\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "\n",
        "    config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\")  # TODO: Optionally download from same location as ckpt and chnage this logic\n",
        "    model = load_model_from_config(config, \"models/ldm/text2img-large/model.ckpt\")  # TODO: check path\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    if opt.plms:\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    prompt = opt.prompt\n",
        "\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "\n",
        "    all_samples=list()\n",
        "    with torch.no_grad():\n",
        "        with model.ema_scope():\n",
        "            uc = None\n",
        "            if opt.scale != 1.0:\n",
        "                uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
        "            for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                c = model.get_learned_conditioning(opt.n_samples * [prompt])\n",
        "                shape = [4, opt.H//8, opt.W//8]\n",
        "                samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                 conditioning=c,\n",
        "                                                 batch_size=opt.n_samples,\n",
        "                                                 shape=shape,\n",
        "                                                 verbose=False,\n",
        "                                                 unconditional_guidance_scale=opt.scale,\n",
        "                                                 unconditional_conditioning=uc,\n",
        "                                                 eta=opt.ddim_eta)\n",
        "\n",
        "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "                for x_sample in x_samples_ddim:\n",
        "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                    Image.fromarray(x_sample.astype(np.uint8)).save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
        "                    base_count += 1\n",
        "                all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "    # additionally, save as grid\n",
        "    grid = torch.stack(all_samples, 0)\n",
        "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "    grid = make_grid(grid, nrow=opt.n_samples)\n",
        "\n",
        "    # to image\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))\n",
        "\n",
        "    print(f\"Your samples are ready and waiting four you here: \\n{outpath} \\nEnjoy.\")\n"
      ],
      "metadata": {
        "id": "TgSjU4lJZhgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0_Gb52UwMHQ"
      },
      "outputs": [],
      "source": [
        "%cd /content/latent-diffusion\n",
        "\n",
        "#@title Generate\n",
        "prompt = 'A sticker of Albert Einstein riding a hors' #@param {type: 'string'}\n",
        "ddim_eta = 0 #@param {type: 'number'}\n",
        "n_samples = 2 #@param {type: 'integer'}\n",
        "n_iter = 4 #@param {type: 'integer'}\n",
        "scale = 5 #@param {type: 'number'}\n",
        "ddim_steps =  50#@param {type: 'integer'}\n",
        "W = 256 #@param {type: 'integer'}\n",
        "H = 256 #@param {type: 'integer'}\n",
        "outdir = 'outputs' #@param {type: 'string'}\n",
        "!mkdir -p $outdir\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "!python txt2img.py --prompt \"$prompt\" --ddim_eta $ddim_eta --n_samples $n_samples --n_iter $n_iter --scale $scale --ddim_steps $ddim_steps --H $H --W $W --outdir $outdir --plms\n",
        "filename = f'{outdir}/{prompt.replace(\" \", \"-\")}.png'\n",
        "im = cv2.imread(filename)\n",
        "cv2_imshow(im)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download images\n",
        "!zip -jrqFS ldm.zip \"$outdir\"\n",
        "from google.colab import files\n",
        "files.download('ldm.zip')"
      ],
      "metadata": {
        "id": "S3PKmI74DENO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}