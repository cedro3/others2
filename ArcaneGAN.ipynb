{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArcaneGAN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/others2/blob/main/ArcaneGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXqfcKRpS5Bi",
        "cellView": "form"
      },
      "source": [
        "#@title インストール\n",
        "#release v0.2\n",
        "!wget https://github.com/Sxela/ArcaneGAN/releases/download/v0.1/ArcaneGANv0.1.jit\n",
        "!wget https://github.com/Sxela/ArcaneGAN/releases/download/v0.2/ArcaneGANv0.2.jit\n",
        "!wget https://github.com/Sxela/ArcaneGAN/releases/download/v0.3/ArcaneGANv0.3.jit\n",
        "!pip -qq install facenet_pytorch\n",
        "\n",
        "# サンプル動画ダウンロード\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=16ei31SsXRqjDM1h6FNeQJALKnbb_huyS', './movies.zip', quiet=False)\n",
        "! unzip movies.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm7x7XgxUUwv",
        "cellView": "form"
      },
      "source": [
        "#@title 初期設定\n",
        "#@markdown Select model version\n",
        "version = '0.3' #@param ['0.1','0.2','0.3']\n",
        "out_x_size = '1280' #@param {type:\"string\"}\n",
        "out_y_size = '720' #@param {type:\"string\"}\n",
        "x_size = int(out_x_size)\n",
        "y_size = int(out_y_size)\n",
        "\n",
        "from facenet_pytorch import MTCNN\n",
        "from torchvision import transforms\n",
        "import torch, PIL\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "mtcnn = MTCNN(image_size=256, margin=80)\n",
        "\n",
        "# simplest ye olde trustworthy MTCNN for face detection with landmarks\n",
        "def detect(img):\n",
        " \n",
        "        # Detect faces\n",
        "        batch_boxes, batch_probs, batch_points = mtcnn.detect(img, landmarks=True)\n",
        "        # Select faces\n",
        "        if not mtcnn.keep_all:\n",
        "            batch_boxes, batch_probs, batch_points = mtcnn.select_boxes(\n",
        "                batch_boxes, batch_probs, batch_points, img, method=mtcnn.selection_method\n",
        "            )\n",
        " \n",
        "        return batch_boxes, batch_points\n",
        "\n",
        "# my version of isOdd, should make a separate repo for it :D\n",
        "def makeEven(_x):\n",
        "  return _x if (_x % 2 == 0) else _x+1\n",
        "\n",
        "# the actual scaler function\n",
        "def scale(boxes, _img, max_res=1_500_000, target_face=256, fixed_ratio=0, max_upscale=2, VERBOSE=False):\n",
        " \n",
        "    x, y = _img.size\n",
        " \n",
        "    ratio = 2 #initial ratio\n",
        " \n",
        "    #scale to desired face size\n",
        "    if (boxes is not None):\n",
        "      if len(boxes)>0:\n",
        "        ratio = target_face/max(boxes[0][2:]-boxes[0][:2]); \n",
        "        ratio = min(ratio, max_upscale)\n",
        "        if VERBOSE: print('up by', ratio)\n",
        "\n",
        "    if fixed_ratio>0:\n",
        "      if VERBOSE: print('fixed ratio')\n",
        "      ratio = fixed_ratio\n",
        " \n",
        "    x*=ratio\n",
        "    y*=ratio\n",
        " \n",
        "    #downscale to fit into max res \n",
        "    res = x*y\n",
        "    if res > max_res:\n",
        "      ratio = pow(res/max_res,1/2); \n",
        "      if VERBOSE: print(ratio)\n",
        "      x=int(x/ratio)\n",
        "      y=int(y/ratio)\n",
        " \n",
        "    #make dimensions even, because usually NNs fail on uneven dimensions due skip connection size mismatch\n",
        "    x = makeEven(int(x))\n",
        "    y = makeEven(int(y))\n",
        "    \n",
        "    size = (x, y)\n",
        "\n",
        "    return _img.resize(size)\n",
        "\n",
        "\"\"\" \n",
        "    A useful scaler algorithm, based on face detection.\n",
        "    Takes PIL.Image, returns a uniformly scaled PIL.Image\n",
        "    boxes: a list of detected bboxes\n",
        "    _img: PIL.Image\n",
        "    max_res: maximum pixel area to fit into. Use to stay below the VRAM limits of your GPU.\n",
        "    target_face: desired face size. Upscale or downscale the whole image to fit the detected face into that dimension.\n",
        "    fixed_ratio: fixed scale. Ignores the face size, but doesn't ignore the max_res limit.\n",
        "    max_upscale: maximum upscale ratio. Prevents from scaling images with tiny faces to a blurry mess.\n",
        "\"\"\"\n",
        "\n",
        "def scale_by_face_size(_img, max_res=1_500_000, target_face=256, fix_ratio=0, max_upscale=2, VERBOSE=False):\n",
        "    boxes = None\n",
        "    boxes, _ = detect(_img)\n",
        "    if VERBOSE: print('boxes',boxes)\n",
        "    img_resized = scale(boxes, _img, max_res, target_face, fix_ratio, max_upscale, VERBOSE)\n",
        "    return img_resized.resize((x_size, y_size))\n",
        "\n",
        "\n",
        "size = 256\n",
        "\n",
        "means = [0.485, 0.456, 0.406]\n",
        "stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "t_stds = torch.tensor(stds).cuda().half()[:,None,None]\n",
        "t_means = torch.tensor(means).cuda().half()[:,None,None]\n",
        "\n",
        "def makeEven(_x):\n",
        "  return int(_x) if (_x % 2 == 0) else int(_x+1)\n",
        "\n",
        "img_transforms = transforms.Compose([                        \n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(means,stds)])\n",
        " \n",
        "def tensor2im(var):\n",
        "     return var.mul(t_stds).add(t_means).mul(255.).clamp(0,255).permute(1,2,0)\n",
        "\n",
        "def proc_pil_img(input_image, model):\n",
        "    transformed_image = img_transforms(input_image)[None,...].cuda().half()\n",
        "            \n",
        "    with torch.no_grad():\n",
        "        result_image = model(transformed_image)[0]; print(result_image.shape)\n",
        "        output_image = tensor2im(result_image)\n",
        "        output_image = output_image.detach().cpu().numpy().astype('uint8')\n",
        "        output_image = PIL.Image.fromarray(output_image)\n",
        "    return output_image\n",
        "\n",
        "#load model\n",
        "model_path = f'/content/ArcaneGANv{version}.jit' \n",
        "in_dir = '/content/in'\n",
        "out_dir = f\"/content/{model_path.split('/')[-1][:-4]}_out\"\n",
        "\n",
        "model = torch.jit.load(model_path).eval().cuda().half()\n",
        "\n",
        "#setup colab interface\n",
        "\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output \n",
        "from IPython.display import display\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def reset(p):\n",
        "  with output_reset:\n",
        "    clear_output()\n",
        "  clear_output()\n",
        "  process()\n",
        " \n",
        "button_reset = widgets.Button(description=\"Upload\")\n",
        "output_reset = widgets.Output()\n",
        "button_reset.on_click(reset)\n",
        "\n",
        "def fit(img,maxsize=512):\n",
        "  maxdim = max(*img.size)\n",
        "  if maxdim>maxsize:\n",
        "    ratio = maxsize/maxdim\n",
        "    x,y = img.size\n",
        "    size = (int(x*ratio),int(y*ratio)) \n",
        "    img = img.resize(size)\n",
        "  return img\n",
        " \n",
        "def show_img(f, size=1024):\n",
        "  display(fit(PIL.Image.open(f),size))\n",
        "\n",
        "def process(upload=False):\n",
        "  os.makedirs(in_dir, exist_ok=True)\n",
        "  %cd {in_dir}/\n",
        "  !rm -rf {out_dir}/*\n",
        "  os.makedirs(out_dir, exist_ok=True)\n",
        "  in_files = sorted(glob(f'{in_dir}/*'))\n",
        "  if (len(in_files)==0) | (upload):\n",
        "    !rm -rf {in_dir}/*\n",
        "    uploaded = files.upload()\n",
        "    if len(uploaded.keys())<=0: \n",
        "      print('\\nNo files were uploaded. Try again..\\n')\n",
        "      return\n",
        "  \n",
        "  in_files = sorted(glob(f'{in_dir}/*'))\n",
        "  for img in tqdm(in_files):\n",
        "    out = f\"{out_dir}/{img.split('/')[-1].split('.')[0]}.jpg\"\n",
        "    im = PIL.Image.open(img)\n",
        "    im = scale_by_face_size(im, target_face=300, max_res=1_500_000, max_upscale=2)\n",
        "    res = proc_pil_img(im, model)\n",
        "    #res = res.resize((1280, 720)) ###resize\n",
        "    res.save(out)\n",
        "\n",
        "  #out_zip = f\"{out_dir}.zip\"\n",
        "  #!zip {out_zip} {out_dir}/*\n",
        "    \n",
        "  processed = sorted(glob(f'{out_dir}/*'))[:3]\n",
        "  for f in processed: \n",
        "    show_img(f, 256)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 動画を静止画にバラす\n",
        "movie = '01.mp4' #@param {type:\"string\"}\n",
        "video_file = '/content/'+movie\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "# flamesフォルダーリセット\n",
        "if os.path.isdir('/content/in'):\n",
        "    shutil.rmtree('/content/in')\n",
        "os.makedirs('/content/in', exist_ok=True)\n",
        " \n",
        "def video_2_images(video_file= video_file,   # ビデオの指定\n",
        "                   image_dir='/content/in/', \n",
        "                   image_file='%s.jpg'):  \n",
        "\n",
        "    # Initial setting\n",
        "    i = 0\n",
        "    interval = 1\n",
        "    length = 3000  # 最大フレーム数\n",
        "    \n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)  # fps取得\n",
        "\n",
        "    while(cap.isOpened()):\n",
        "        flag, frame = cap.read()  \n",
        "        if flag == False:  \n",
        "                break\n",
        "        if i == length*interval:\n",
        "                break\n",
        "        if i % interval == 0:    \n",
        "           cv2.imwrite(image_dir+image_file % str(int(i/interval)).zfill(6), frame)\n",
        "        i += 1 \n",
        "    cap.release()\n",
        "    return fps, i, interval\n",
        " \n",
        "fps, i, interval = video_2_images()\n",
        "print('fps = ', fps)\n",
        "print('flames = ', i)\n",
        "print('interval = ', interval)"
      ],
      "metadata": {
        "id": "ox5OqfitQiby",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdePnlXFX7x8",
        "cellView": "form"
      },
      "source": [
        "#@title 静止画をアニメに変換\n",
        "process()\n",
        "%cd ..\n",
        "\n",
        "# コード内でカレントディレクトリを/content/inに移しているので、最後に/contentに戻す\n",
        "# そうしないと、動画から静止画をバラすときに/content/inを一旦削除するためカレントディレクトリを見失うため"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title アニメから動画を作成\n",
        "\n",
        "# リセットファイル\n",
        "if os.path.exists('/content/output.mp4'):\n",
        "   os.remove('/content/output.mp4')\n",
        "\n",
        "if version == '0.1':\n",
        "  ! ffmpeg -r $fps -i /content/ArcaneGANv0.1_out/%06d.jpg -vcodec libx264 -pix_fmt yuv420p /content/output.mp4\n",
        "if version == '0.2':\n",
        "  ! ffmpeg -r $fps -i /content/ArcaneGANv0.2_out/%06d.jpg -vcodec libx264 -pix_fmt yuv420p /content/output.mp4\n",
        "if version == '0.3':\n",
        "  ! ffmpeg -r $fps -i /content/ArcaneGANv0.3_out/%06d.jpg -vcodec libx264 -pix_fmt yuv420p /content/output.mp4"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j61Ga1xRFzbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 動画の再生\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('/content/output.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"70%\" height=\"70%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jF3YY8DOAghg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}