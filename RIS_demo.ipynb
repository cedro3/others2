{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RIS_demo",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/others2/blob/main/RIS_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRFbyn-ay_Ei"
      },
      "source": [
        "# セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGqP06Rtm9TE"
      },
      "source": [
        "# githubからコードをコピー\n",
        "!git clone https://github.com/mchong6/RetrieveInStyle.git\n",
        "%cd RetrieveInStyle\n",
        "\n",
        "# ライブラリーのインストール\n",
        "!pip install tqdm gdown scikit-learn scipy lpips dlib opencv-python\n",
        "\n",
        "# ライブラリーのインポート\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "cudnn.benchmark = True\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from model import *\n",
        "from spherical_kmeans import MiniBatchSphericalKMeans as sKmeans\n",
        "from tqdm import tqdm as tqdm\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) # get rid of interpolation warning\n",
        "from util import *\n",
        "from google.colab import files\n",
        "from util import align_face\n",
        "import os\n",
        "from e4e_projection import projection\n",
        "%matplotlib inline\n",
        "\n",
        "# 学習済みモデルのロード\n",
        "device = 'cuda' # if GPU memory is low, use cpu instead\n",
        "generator = Generator(1024, 512, 8, channel_multiplier=2).to(device).eval()\n",
        "ensure_checkpoint_exists('stylegan2-ffhq-config-f.pt')\n",
        "ckpt = torch.load('stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "with torch.no_grad():\n",
        "    mean_latent = generator.mean_latent(50000)\n",
        "\n",
        "# カタログのロード\n",
        "truncation = 0.5\n",
        "stop_idx = 11 # choose 32x32 layer to do kmeans clustering\n",
        "n_clusters = 18 # Number of Kmeans cluster\n",
        "clusterer = pickle.load(open(\"catalog.pkl\", \"rb\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCxun63Nm9TJ"
      },
      "source": [
        "# クラスタリングの視覚化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Pm31p6m9TK"
      },
      "source": [
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_z = torch.randn([1, 512]).to(device)\n",
        "    sample_w = generator.get_latent(sample_z, truncation=truncation, mean_latent=mean_latent)\n",
        "    sample, outputs = generator(sample_w, is_cluster=1) # [b, c, h, w]\n",
        "\n",
        "# obtain 32x32 activations and classify using kmeans\n",
        "act = flatten_act(outputs[stop_idx][0])\n",
        "b,c,h,w = outputs[stop_idx][0].size()\n",
        "\n",
        "alpha = 0.5\n",
        "seg_mask = clusterer.predict(act)\n",
        "seg_mask = torch.from_numpy(seg_mask).view(1,h,w)\n",
        "seg_out = decode_segmap(seg_mask)\n",
        "\n",
        "sample_d = F.interpolate(sample, size=(256,256), mode='bilinear').cpu()\n",
        "seg_out_d = F.interpolate(seg_out, size=(256,256), mode='nearest')\n",
        "out = alpha*seg_out_d + (1-alpha)*sample_d\n",
        "\n",
        "display_image(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISOQPp-bm9TK"
      },
      "source": [
        "# 顔の特徴のラベル付けと関数定義\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2IPbbs4m9TM"
      },
      "source": [
        "#Gives an index to each feature we care about\n",
        "labels2idx = {\n",
        "    'nose': 0,\n",
        "    'eyes': 1,\n",
        "    'mouth':2,\n",
        "    'hair': 3,\n",
        "    'background': 4,\n",
        "    'cheeks': 5,\n",
        "    'neck': 6,\n",
        "    'clothes': 7,\n",
        "}\n",
        "\n",
        "# Assign to each feature the cluster index from segmentation\n",
        "labels_map = {\n",
        "    0: torch.tensor([7]),\n",
        "    1: torch.tensor([1,6]),\n",
        "    2: torch.tensor([4]),\n",
        "    3: torch.tensor([0,3,5,8,10,15,16]),\n",
        "    4: torch.tensor([11,13,14]),\n",
        "    5: torch.tensor([9]),\n",
        "    6: torch.tensor([17]),\n",
        "    7: torch.tensor([2,12]),\n",
        "}\n",
        "\n",
        "idx2labels = dict((v,k) for k,v in labels2idx.items())\n",
        "n_class = len(labels2idx)\n",
        "\n",
        "\n",
        "# compute M given a style code.\n",
        "@torch.no_grad()\n",
        "def compute_M(w, device='cuda'):\n",
        "    M = []\n",
        "    \n",
        "    # get segmentation\n",
        "    _, outputs = generator(w, is_cluster=1)\n",
        "    cluster_layer = outputs[stop_idx][0]\n",
        "    activation = flatten_act(cluster_layer)\n",
        "    seg_mask = clusterer.predict(activation)\n",
        "    b,c,h,w = cluster_layer.size()\n",
        "\n",
        "    # create masks for each feature\n",
        "    all_seg_mask = []\n",
        "    seg_mask = torch.from_numpy(seg_mask).view(b,1,h,w,1).to(device)\n",
        "    \n",
        "    for key in range(n_class):\n",
        "        # combine masks for all indices for a particular segmentation class\n",
        "        indices = labels_map[key].view(1,1,1,1,-1) \n",
        "        key_mask = (seg_mask == indices.to(device)).any(-1) #[b,1,h,w]\n",
        "        all_seg_mask.append(key_mask)\n",
        "        \n",
        "    all_seg_mask = torch.stack(all_seg_mask, 1)\n",
        "\n",
        "    # go through each activation layer and compute M\n",
        "    for layer_idx in range(len(outputs)):\n",
        "        layer = outputs[layer_idx][1].to(device)\n",
        "        b,c,h,w = layer.size()\n",
        "        layer = F.instance_norm(layer)\n",
        "        layer = layer.pow(2)\n",
        "        \n",
        "        # resize the segmentation masks to current activations' resolution\n",
        "        layer_seg_mask = F.interpolate(all_seg_mask.flatten(0,1).float(), align_corners=False, \n",
        "                                     size=(h,w), mode='bilinear').view(b,-1,1,h,w)\n",
        "        \n",
        "        masked_layer = layer.unsqueeze(1) * layer_seg_mask # [b,k,c,h,w]\n",
        "        masked_layer = (masked_layer.sum([3,4])/ (h*w))#[b,k,c]\n",
        "\n",
        "        M.append(masked_layer.to(device))\n",
        "\n",
        "    M = torch.cat(M, -1) #[b, k, c]\n",
        "    \n",
        "    # softmax to assign each channel to a particular segmentation class\n",
        "    M = F.softmax(M/.1, 1)\n",
        "    # simple thresholding\n",
        "    M = (M>.8).float()\n",
        "    \n",
        "    # zero out torgb transfers, from https://arxiv.org/abs/2011.12799\n",
        "    for i in range(n_class):\n",
        "        part_M = style2list(M[:, i])\n",
        "        for j in range(len(part_M)):\n",
        "            if j in rgb_layer_idx:\n",
        "                part_M[j].zero_()\n",
        "        part_M = list2style(part_M)\n",
        "        M[:, i] = part_M\n",
        "\n",
        "    return M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojscea08whhf"
      },
      "source": [
        "# 【オプション】画像から潜在変数を求める\n",
        "＊自分で用意した画像を使わない場合はこのブロックの実行をパスして下さい。\\\n",
        "＊自分で用意した画像（jpg）から顔部分を切り取って潜在変数化したい場合は、PCからその画像をドラッグ＆ドロップで RetrieveInStyle/images へアップロードして（複数OK）から、下記を実行して下さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uPnUk4-vnC2"
      },
      "source": [
        "import glob\n",
        "files = glob.glob('images/*.jpg')\n",
        "for file in files:\n",
        "   filename = file[7:-4]\n",
        "   cropped_face = align_face(file)  # 顔部分の切り取り\n",
        "   projection(cropped_face, filename, generator, device)  # 潜在変数の取得"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Mxkkgwm9TN"
      },
      "source": [
        "# 顔の特徴の転送"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azXsVoBAm9TN"
      },
      "source": [
        "# ソース画像と参照画像の設定\n",
        "plt.rcParams['figure.dpi'] = 75\n",
        "\n",
        "# load codes from inverted real images using our projection code\n",
        "with torch.no_grad():\n",
        "    '''\n",
        "    if you gan inverted in the previous cell, you can call it here with variable filename\n",
        "    otherwise, you can randomly generate or call a pre-inverted image\n",
        "    '''\n",
        "    # source = load_source([filename], generator, device)\n",
        "    source = load_source(['brad_pitt'], generator, device)\n",
        "    source_im, _ = generator(source)\n",
        "    display_image(source_im, size=256)\n",
        "    \n",
        "    ref = load_source(['emma_watson', 'emma_stone', 'jennie'], generator, device)\n",
        "    ref_im, _ = generator(ref)\n",
        "    ref_im = downsample(ref_im)\n",
        "    \n",
        "    show(normalize_im(ref_im).permute(0,2,3,1).cpu(), title='References')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvF4GLTum9TN",
        "scrolled": false
      },
      "source": [
        "# 顔の特徴の転送\n",
        "# Compute M for both source and reference images use cpu here to save memory\n",
        "source_M = compute_M(source, device='cpu')\n",
        "ref_M = compute_M(ref, device='cpu')\n",
        "\n",
        "# Find relevant channels for source and reference by taking max over their individual M\n",
        "max_M = torch.max(source_M.expand_as(ref_M), ref_M)\n",
        "max_M = add_pose(max_M, labels2idx)\n",
        "\n",
        "all_im = {}\n",
        "\n",
        "with torch.no_grad():    \n",
        "    # features we are interest in transferring\n",
        "    parts = ('eyes', 'nose', 'mouth', 'hair','pose')\n",
        "    for label in parts:\n",
        "        if label == 'pose':\n",
        "            idx = -1\n",
        "        else:\n",
        "            idx = labels2idx[label]\n",
        "            \n",
        "        part_M = max_M[:,idx].to(device)\n",
        "        blend = style2list(add_direction(source, ref, part_M, 1.3))\n",
        "            \n",
        "        blend_im, _ = generator(blend)\n",
        "        blend_im = downsample(blend_im).cpu()\n",
        "        all_im[label] = normalize_im(blend_im)\n",
        "        \n",
        "part_grid(normalize_im(source_im.detach()), normalize_im(ref_im.detach()), all_im);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-qa0pfQm9TO"
      },
      "source": [
        "# 顔の特徴の転送（度合いのコントロール）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpbNadBpm9TO",
        "scrolled": false
      },
      "source": [
        "# ソース画像と参照画像の設定\n",
        "plt.rcParams['figure.dpi'] = 75\n",
        "torch.manual_seed(3913)\n",
        "    \n",
        "with torch.no_grad():\n",
        "    source = load_source(['emma_stone'], generator, device)\n",
        "    source_im, _ = generator(source)\n",
        "    display_image(source_im, size=256)\n",
        "    \n",
        "    ref = load_source(['brad_pitt'], generator, device)\n",
        "    ref_im, _ = generator(ref)\n",
        "    ref_im = downsample(ref_im)\n",
        "    display_image(ref_im, title='reference')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixo9fpeRm9TO"
      },
      "source": [
        "# 顔の特徴の転送（度合いのコントロール）\n",
        "plt.rcParams['figure.dpi'] = 200 \n",
        "\n",
        "source_M = compute_M(source, device='cpu')\n",
        "ref_M = compute_M(ref, device='cpu')\n",
        "\n",
        "max_M = torch.max(source_M.expand_as(ref_M), ref_M)\n",
        "max_M = add_pose(max_M, labels2idx)\n",
        "\n",
        "labels = ('eyes', 'hair') # choose what feature to interpolate {eyes/nose/mouth/hair/pose}\n",
        "max_alpha = 1.5 # max range to interpolate\n",
        "\n",
        "all_im = []\n",
        "with torch.no_grad():    \n",
        "    for label in labels:\n",
        "        row = []\n",
        "        \n",
        "        if label == 'pose':\n",
        "            idx = -1\n",
        "        else:\n",
        "            idx = labels2idx[label]\n",
        "\n",
        "        for alpha in np.linspace(-max_alpha, max_alpha, 5):\n",
        "            part_M = max_M[:,idx].to(device) \n",
        "            blend = style2list(add_direction(source, ref, part_M, alpha))\n",
        "            blend_im, _ = generator(blend)\n",
        "            blend_im = downsample(blend_im).cpu()\n",
        "            row.append(blend_im)\n",
        "\n",
        "        row.append(ref_im.cpu())\n",
        "        row = torch.cat(row, -1)\n",
        "        all_im.append(row)\n",
        "        \n",
        "    all_im = torch.cat(all_im, 2)\n",
        "    display_image(all_im, size=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UDXOQ55m9TP"
      },
      "source": [
        "# 顔の特徴検索\n",
        "GANで5000個の顔のデータベースを作成し、顔の特徴が似た人を抽出する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN-O34QCm9TP"
      },
      "source": [
        "# 顔データベースの作成\n",
        "torch.manual_seed(12390)\n",
        "num_data = 5000\n",
        "dataset = torch.randn([num_data, 512]).to(device)\n",
        "with torch.no_grad():\n",
        "    dataset_w = generator.get_latent(dataset, truncation=truncation, mean_latent=mean_latent)\n",
        "    dataset_M = []\n",
        "    for i in tqdm(range(num_data)):\n",
        "        # have to use cuda for this or it will be very slow\n",
        "        dataset_M.append(compute_M(index_layers(dataset_w, i), device='cuda'))\n",
        "\n",
        "    dataset_M = remove_2048(torch.cat(dataset_M, 0), labels2idx).to(device) #[N, K, C]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUXmlLzam9TQ"
      },
      "source": [
        "# 検索対象の表示\n",
        "plt.rcParams['figure.dpi'] = 75 \n",
        "\n",
        "with torch.no_grad():\n",
        "    query_w = load_source(['tom_hiddleston'], generator, device)\n",
        "    \n",
        "    query_im, _ = generator(query_w)\n",
        "    display_image(query_im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tmBV9eYm9TQ",
        "scrolled": false
      },
      "source": [
        "# 検索実行と結果表示\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "  \n",
        "num_nn = 6\n",
        "all_im = []\n",
        "query_M = remove_2048(compute_M(query_w, device=device), labels2idx).to(device)\n",
        "\n",
        "r_query_w = list2style(query_w)\n",
        "r_dataset_w = list2style(dataset_w)\n",
        "\n",
        "# normalize each style dimension\n",
        "largest = r_dataset_w.abs().max(0, keepdim=True)[0] + 1e-8\n",
        "norm_query_w = r_query_w/largest\n",
        "norm_target_w = r_dataset_w/largest\n",
        "\n",
        "# choose what features to perform retrieval on\n",
        "# parts = ('eyes', 'nose', 'mouth', 'hair')\n",
        "parts = ('eyes', 'mouth', 'hair',)\n",
        "\n",
        "# perform cosine similarity w.r.t a given feature\n",
        "with torch.no_grad():\n",
        "    for part in parts:\n",
        "        idx = labels2idx[part]\n",
        "        \n",
        "        source_part = norm_query_w * query_M[:,idx].to(device)\n",
        "        target_part = norm_target_w * dataset_M[:,idx].to(device)\n",
        "        \n",
        "        distance = cos_dist(target_part, source_part)  \n",
        "        nearest_neighbors = torch.sort(distance)[1][:num_nn]\n",
        "\n",
        "        row = [query_im.cpu()]\n",
        "        for idx in nearest_neighbors:\n",
        "            nn_w = index_layers(dataset_w, int(idx))\n",
        "            nn_image, _ = generator(nn_w)\n",
        "            row.append(nn_image.cpu())\n",
        "        row = [downsample(a) for a in row]\n",
        "        row = torch.cat(row, -1)\n",
        "        all_im.append(row)\n",
        "        \n",
        "    all_im = torch.cat(all_im,-2)\n",
        "    display_image(all_im, size=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}